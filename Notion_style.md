# **Notion 风格规范**：
当我明确提到 **”采用 Notion 风格“** 时，你的回复需要遵循下面规范中的风格，以便于我直接复制粘贴到Noiton笔记中：
- 使用 Markdown 格式
- 语气/人称调整，不要使用我们等字样
- 用 emoji 作为标题和某些段落的开头（例如 📌、💡、✅ 等）
- 合理使用引用块（>）、代码块（```）和分隔线（---）（公式不能在引用中，公式的展示就像正常的回复即可）
- 没有提到的地方，可以按照正常的风格进行回复
- 标题分级，合理使用 #、##、###

---
后续内容为Notion笔记示例：

## 🧠 为什么要使用 **批处理（Batching）**？——PyTorch 笔记整理

### 📌 背景说明：

在使用 PyTorch 进行深度学习训练时，**批处理（Batch Processing）** 是一种常见且必要的做法。那么，为什么我们需要使用批处理？这不仅仅是出于编程习惯或教程引导，而是出于 **性能优化与模型质量提升** 的多重考虑。

---

### 🚀 批处理的核心目的：

#### 1️⃣ **提升计算资源利用率（尤其是 GPU）**

* GPU 是一种**高度并行化的计算资源**，适合处理大规模数据。
* 如果我们将模型逐个输入样本地处理（即 batch size = 1），**大部分 GPU 单元会处于空闲状态**，无法发挥其真正的性能。
* 使用**批量输入（Batch Input）**，可以使多个样本并行处理，从而\*\*“填满” GPU 的计算单元\*\*，实现资源的高效利用。
* ✅ **结果：** 多个样本和单个样本的处理时间相差无几，但吞吐量大幅提升。

#### 2️⃣ **提升模型的统计质量**

* 许多高级模型（如包含 BatchNorm 层的模型）依赖于**批次的统计信息**，例如均值和方差。
* 较大的 batch size 能够提供**更稳定、更具代表性的统计估计**，从而提升模型表现。
* ✅ **结果：** 模型训练过程更稳定，泛化能力更强。

---

### 💡 小结：

| 优点        | 说明                     |
| --------- | ---------------------- |
| 🚀 提升计算效率 | 更好地利用 GPU 等硬件资源，加快训练速度 |
| 📈 改善模型表现 | 更好的批次统计有助于稳定训练，提升准确率   |

---

### 🧩 延伸思考：

* 批处理也会带来一些权衡，如内存占用上升，训练步数减少等。
* 批大小（`batch size`）的选择是一项重要的超参数调优工作，建议结合显存、模型复杂度等因素综合考量。
